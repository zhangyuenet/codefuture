# 作业说明
## 目录
1. 作业的意图；
2. 京东爬虫的代码解释；
3. 完成作业的思路。

## 作业
作业2：尝试正确运行jd.py（提示，需要安装某个依赖组件，仔细阅读代码，查看错误记录，了解需要安装的组件是什么），这是个很有意思的程序，输入任意商品类型的名称（例如“手机，电视”等），都可以自动从京东网站获取此类商品的详情和价格，并输出成表格。首先要能够正确运行这个程序，然后尝试添加一点代码，在输出的表格中增加一列，标明每个商品是否包含赠品。（提示，代码中给出了爬虫抓取的是京东的那个页面，尝试访问一下这个页面，可以看到如果商品有赠品，会在商品的右下角标注一个“赠”的图标）


## 作业的意图
这个程序是原本希望你们独立完成的程序，但是考虑到大家的时间精力以及学习基础。我们改成了“完形填空”的形式。
这个程序的功能是从京东网站获取某类商品的信息。此类获取商品信息的爬虫是现实世界中非常常见也非常有用的。大家设想一下，如果能写出京东的爬虫，再写出天猫的爬虫，拼多多的爬虫。。。是否就可以有一个比价功能了？一台新的iPhone手机到底在哪家最便宜呢？
这个程序如果能搞的比较明白，就可以很轻松的改造出针对天猫的爬虫、拼多多的爬虫，等等，这样，距离写一个比价工具就非常接近了。
该程序特意用到了一个新的组件：BeautifulSoup，最主要的功能是从网页抓取数据。它的具体功能在这个程序里面有演示，大家“望文生义”的了解和模仿即可使用。如果发现不明白的，搜索引擎是最好的老师。
这次的作业的目的：
1. 希望大家首先注意到BeautifulSoup这个工具的用法，保存起来，将来可以“照猫画虎”的使用。
2. 其次，希望大家复习从页面分析数据的方法，体会一下代码中展现的分析方法；
3. 如果能够读懂第二步，就能清楚的知道，要增加更多的元素应该怎么做。作业要求的是抓取商品是否包含赠品，这要求大家分析京东的商品页面，并分析出是否有赠品的标志是什么。这考验大家分析HTML内容的能力；
4. 仿照第二步的代码，增加新的代码，获取赠品信息。

如果大家能够完成，说明大家基本掌握了：
1. 爬虫的基本原理；
2. 分析HTML页面；
3. BeautifulSoup的基本用法；
4. 具备从其他网站爬取数据的能力。

## 代码解释

首先大家需要让这个程序跑起来，第一次运行大概会出错误。提示找不到BeautifulSoup，大家这时候应该清楚需要安装组件了。大家知道用pip，但是安装组件的准确名称是什么呢？在这里不直接告诉大家，去网上搜索一下“安装BeautifulSoup”。

```python
    goods = input('请输入要搜索的物品：')  
    yeshu = int(input('请输入要查询到的页数:'))  
    ulist = []  
    for i in range(yeshu+1):  
        try:  
            if i != 0:  
                text = gethtml(goods,i)  
                findhtml(text , ulist)  
            savehtml(goods , ulist)  
        except:  
            break  
```
这是主程序，代码比较容易解释。京东的商品搜索页面，支持一次显示多页。因此指定页数后，就可以获取多页数据。
主程序除了提供输入让用户指定商品名称和页数之外，主要功能就是三个：
1. gethtml()获取商品页面，并存入text。
2. findhtml()从text中提取需要的商品信息；
3. 待所有页面提取完成后，savehtml()保存成“商品名”的csv文件保存起来。这个文件，可以用excel打开，也可以用文本编辑器打开。
保存的代码savehtml()比较简单。我们介绍一下gethtml()和findhtml()这两个函数。
```python
def gethtml(kind,page):  
    #'''''获取url请求'''  
    header = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko)"}
    pagenum = str(2 * page)  
    try:  
        r = requests.get('https://search.jd.com/Search?keyword=' + kind + '&enc=utf-8&page=' + pagenum, headers=header)#链接url  
        r.raise_for_status() 
        r.encoding = r.apparent_encoding#转码  
        print('爬取第{}页：'.format(page))  
        #print(r.text)
        return r.text#返回html  
    except:  
        print('链接异常！！！')  
        return ''  
```

这段代码告诉我们的核心信息是，我们爬取数据的页面链接类似于这样：
```python
https://search.jd.com/Search?keyword=手机&enc=utf-8&page=1
```
这很重要，后面分析是否有赠品时，需要手动输入这个链接地址。
其他的部分，大家都学过，只有一个知识点，大家注意：
```python
r.raise_for_status()
```
这句代码的意思是，如果获取数据的返回值不是正常值（在HTTP中，200代表正常，其他则是各种异常状态），则抛出一个错误。错误会被except捕获到。显示出”链接异常”并结束程序。这是个很好的写法，防止页面获取时有错误发生。大家值得借鉴。

再来看关键的findhtml代码：
```python
#获取定位资源  
def findhtml(html,httplist):  
    #"""寻找资源"""  
    soup = BeautifulSoup(html,'lxml')  
    links = soup.find_all('div', class_='gl-i-wrap')#寻找'div'标签  
    print(len(links))
    for link in links:  
        ui = []  
        namediv = link.find('div', class_='p-name p-name-type-2')#寻找商品名称和链接  
        title = namediv.a['title'] 
        ui.append(title)#名称加入到ui中  

        href = namediv.a['href']  
        if 'https:' not in href:#添加链接  
            ui.append('https:' + href)  
        else:  
            ui.append(href) 

        pricediv = link.find('div', class_='p-price')#寻找商品价格  
        try:  
            price =  pricediv.strong.i.text 
            ui.append(price)#价格加入到ui中  
        except:  
            ui.append('')  
        
        aggressmentdiv = link.find('div', class_='p-commit')#寻找评论 
        commit = aggressmentdiv.strong.a['href']  
        ui.append(commit)#评论链接添加到ui中  
        
        httplist.append(ui)  
  
```
这段代码简化一下，大致包含这些部分：
```python
def findhtml(html,httplist):  
    #初始化一个beautifulsoup对象。
    soup = BeautifulSoup(html,'lxml')  
    # 获取所有商品列表
	links = soup.find_all('div', class_='gl-i-wrap')#寻找'div'标签  
    
    for link in links:  
        ui = [] # 每个商品的数据放入一个ui对象。  
       
		# 商品名称和链接在一起，找到对应的标签，存入名称title和链接href。
		。。。
		# 寻找商品价格，存入price
        。。。
		# 寻找评论链接，存入commit。
		。。。
		# 这时候，一个商品的信息就全部存入ui了：
		# ui = [title名称，href链接，price价格，commit评论]    
		# 将包含一个商品信息的名称为ui的list放入到名为httplist的list中。
        httplist.append(ui)  
```
所以，大家的关注点应该是如何获取这四个数据。在这里，以名称为例子：
大家打开浏览器（必须是以下之一：IE/Safari/Chrome/firefox/Edge），输入上面我们提到的链接：
```python
https://search.jd.com/Search?keyword=手机&enc=utf-8&page=1
```
打开“开发者模式”，用inspect工具去选取商品名称，如图所示：
![][image-1]
就能找到一个商品对应的页面结构是这样的（老师写在这里的是忽略了大部分详细内容的框架结构）：
```html
<div class="gl-i-wrap">
		<div class="p-img"></div>
		<div class="p-scroll"></div>
		<div class="p-price"></div>
		<div class="p-name p-name-type-2">
			<a target="_blank" title="柔性屏折叠手机，折叠下一个十年！12期免息，快来抢购！" href="https://item.jd.com/46492972183.html"
			</a>
		</div>
		<div class="p-commit" data-done="1"></div>
		<div class="p-focus"></div>
		<div class="p-shop"></div>
	</div>
```
往下看看，其他商品的结构也是这样的。所以，只需要找到“gl-i-wrap”就可以找到一个商品。因此：
```python
    #初始化一个beautifulsoup对象。输入的html就是上一步获得的完整页面内容。
    soup = BeautifulSoup(html,'lxml')  
    # 获取所有商品列表
	links = soup.find_all('div', class_='gl-i-wrap')#寻找'div'标签 
```
Links就包含了所有商品的信息。
详细信息的分析就是对links中每一个元素进行分析了：
```python
 for link in links:  
```
在一个商品内部，我们能看出来，商品的名称是在“p-name p-name-type-2”下。
因此有了这样的代码：
```python
 namediv = link.find('div', class_='p-name p-name-type-2')#寻找商品名称和链接  
```
Namediv下的超链接标签\<a\>中，包含了我们所需的商品名称title，还包含了我们所需的商品链接href。
因此写法是：
```python
	title = namediv.a['title'] 
    ui.append(title)#名称加入到ui中  

    href = namediv.a['href']  
    if 'https:' not in href:#添加链接  
        ui.append('https:' + href)  
    else:  
        ui.append(href) 

```
大家有兴趣的话，可以想象，按照我们之前学的，如果不用beautifulsoup，应该怎么取数据？实际上，因为又了beautifulsoup，让我们避免了去写复杂的XPath，这才是这个组件最大的作用。
下面的if else功能一看就明白了，如果链接地址不包含https:开头，则给他加上，这样链接地址看上去都是完整的https://xxxxxx的样子。

下面的价格price和评论commit是一样的处理方式。大家仔细看看就可以明白了。

到这里，大家对代码应该熟悉了，下面说说完成作业的思路。

## 作业思路
要想知道有没有赠品，需要干这些事情：
1. 观察网页，看看商品有没有赠品的标志是什么？
2. 仿照上面介绍的获取姓名价格的方式，获取是否有赠品的标志；
3. 修改保存数据的代码，因为完成作业时，不再是四列（名称，链接，价格，评价），而是五列（名称，链接，价格，评价，是否有赠品）。

### 观察网页
继续观察上面提到的网页，注意如图标红的地方：
![][image-2]
页面中，凡是有赠品的地方，都会有一个“赠”的图标。对应在代码中，他存在于下面的结构中：
```python
<div class="p-icons" >
			<i class="goods-icons J-picon-tips J-picon-fix" data-idx="1" data-tips="京东自营，品质保障">自营</i>
    		<i class="goods-icons4 J-picon-tips" style="border-color:#4d88ff;color:#4d88ff;" data-idx="1" data-tips="品质服务，放心购物">放心购</i>
			<i class="goods-icons4 J-picon-tips J-picon-fix" data-tips="天天低价，正品保证">秒杀</i>
<i class="goods-icons4 J-picon-tips" data-tips="购买本商品送赠品">赠</i>		</div>
```

现在清楚了，只需要找到名称为“p-icons”的div。看看里面有没有一个\<i\>\</i\>标签的内容是“赠”就可以了。这个代码怎么写，留给同学们自己去想。

最后修改保存的代码相对容易：
```python
    with open(goods + '.csv','w+') as f:  
        writer = csv.writer(f)  
        writer.writerow(['商品','链接','价格','评价'])  
        for u in range(len(ul)):  
            if ul[u]:  
                writer.writerow([ul[u][0],ul[u][1],ul[u][2],ul[u][3]])
```
在writer.writerow()这里多写一列“赠品”就可以了。


[image-1]:	jdpage.png
[image-2]:	jdpage2.png