# 爬虫2
## 教学目标
- 爬虫内寻找特定目标（获得最大页码）
- 代码输出CSV；
- 代码输出HTML；

## 课程导入
上节课我们学到了利用爬虫获取电影的评分数据。基本上，只要让这个爬虫持续运行，不管这些电影的评分有什么变化，或者排名有什么变化，我们都能第一时间知道。现在首先提一个问题：

【学生提问】如何让我们的爬虫持续运行，从而达到监控网站数据变化的作用？
【教师参考】可以引导学生回忆之前写服务器代码时，使用的 `while True`，当然，如果学生利用OS的自动任务功能或者用脚本循环运行，也是很好的办法。

上节课的代码中，有一个缺陷，我们作为思考题留给了大家，有多少人想到了办法呢？

> 我们在课程中固定的只取了十页数据，想想看，结合最后一部分分析页面的方法。可否通过页面提供的信息，让程序知道网站一共有多少个页面，并且将这些页面的链接地址都取到？

今天我们先来解决这个问题。

## 爬虫内设定范围（获取最大页码）
我们还是先来观察这个页面（写爬虫，观察是非常重要的，能够观察越多的信息，你的爬虫就会越有效，越简单）：
[https://maoyan.com/films?showType=3&sortId=3][1]

【学生提问】大家发现最大页面在哪里了么？
【教师参考】回答正确！最大页面就是最下面的28312！

【学生提问】接着问大家，上节课我们说过，链接地址的规律是：第N页的地址 offset=N乘以30，N从0开始。那么大家猜猜看，最后这个页面的链接地址是什么：
[https://maoyan.com/films?showType=3&sortId=3&offset=？？？？？][2]
【教师参考】正确的答案是：
[https://maoyan.com/films?showType=3&sortId=3&offset=849330][3]
算法是offset = (28312 - 1) X 30 = 849330

实际上也可以不算，只需要用鼠标在28312的按钮上点击右键，选择“复制链接地址”就可以看到了，当然，如果选择查看源代码，就可以看到下面的代码了：
```html
<a class="page_28312"
      href="?showType=3&amp;sortId=3&amp;offset=849330"
  >28312
</a>

```

好了，有了这些信息，大家对着下面的代码，快速的想想，该怎么改造，就可以自动的获取所有页面的信息：
```python
import requests

url = 'https://maoyan.com/films?showType=3&sortId=3&offset='
header = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko)"}
html = requests.get(url, headers=header).text
for i in range(0, 9):
	p_url = url + str(i * 30)
	print(p_url)
```

虽然有些复杂，但是老师带着大家仔细数一下（这个过程可以帮助大家更加熟悉HTML结构），XPath路径是：
```python
/html/body/div[4]/div/div[2]/div[3]/ul/li[7]/a/text()
```
看上去很吓人，但是只要仔细数，不断测试，就一定能找到。
【学生提问】大家想想，我们为什么不这样写，这样写岂不是更简单：
```python
div[@class="page_28312"]/text()
```

【教师参考】如果我们用class名称，如果页面数发生变化，这个class名称和页面数相关，因此大概率名称也会随之变化，我们做这件事的目的就是为了防止页面数变化，相对而言，页面导航的结构改变的几率要小于页面数变化的概率。所以我们选择用路径寻找最大页面数，而不是直接用class name。

这样我们的代码就要改造成这样了（maxpage.py）：
```python
import requests
from lxml import etree

url = 'https://maoyan.com/films?showType=3&sortId=3&offset='
header = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko)"}
html = requests.get(url, headers=header).text
data = etree.HTML(html)
maxpage = data.xpath('/html/body/div[4]/div/div[2]/div[3]/ul/li[7]/a/text()')
for i in range(1, int(maxpage) + 1):
	p_url = url + str(i * 30)
	print(p_url)
```


最后，老师给大家教如何“投机取巧”的做到。
在Chrome中，打开“开发者模式”，在Elements标签中，找到对应的元素，右键选择Copy - “Copy XPath”，就能得到如下的地址：
```python
//*[@id="app"]/div/div[2]/div[3]/ul/li[7]/a
```
这是个相对地址，也可以把前面id=app之前的部分替换成绝对路径。
有兴趣的同学，可以自己搜索一下，其他浏览器都有内置的工具，或者一些插件允许用户直接复制出xpath，大大简化我们的工作。请大家记住，以后你在写代码的过程中，**如果发现某件事情很耗功夫，那么一定请上网去搜索一下，99.99%的情况是，早已有人设计了某个工具来方便的解决这个问题。**

好了，到这里，我们这个爬虫程序就差不多了，但是追求完美的我们当然还是可以吹毛求疵的。现在，老师认为我们爬虫程序的最大缺陷是，没有办法把我们运行的数据保存起来。关闭命令行窗口，所有的数据就不见了。在这里，我们给大家介绍一个特别有用的文件格式：CSV。

## CSV
大家首先看看CSV的定义：
[https://baike.baidu.com/item/CSV/10739][4]
或者：
[https://zh.wikipedia.org/wiki/%E9%80%97%E5%8F%B7%E5%88%86%E9%9A%94%E5%80%BC][5]
光看定义，有些复杂，我们简单一点写一个简单的文件：
```bash
name,age,city
Tom,12,Beijing
Jerry,13,Toronto
Mike,10,London
```
这个文件看上去平淡无奇，但是我们保存起来，大家试试看打开它。
我们竟然可以用Excel和Number（仅Mac电脑），或者WPS打开它！因为这实质上是个表格。是不是很方便？
我们简单的处理一下上节课学的程序，为了节省时间，我们还是沿用上节课只获取第一页的设定（info2csv.py）。
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
from lxml import etree
from fake_useragent import UserAgent

url = 'https://maoyan.com/films?showType=3&sortId=3&offset='
ua = UserAgent()
header = {"User-Agent": ua.random}
html = requests.get(url, headers=header).text
# 名称，评分，详情
data = etree.HTML(html)
names = data.xpath('//div[@class="channel-detail movie-item-title"]/@title')
infos = data.xpath('//div[@class="channel-detail movie-item-title"]/a/@href')

#print(len(names), names)
#print(len(infos), infos)

scores = data.xpath('//div[@class="channel-detail channel-detail-orange"]')
scors = [x.xpath('string(.)') for x in scores]
#print(len(scors), scors)


f = open('movielist.csv', 'w')
f.write('name,url,score\r\n')
for i in range(0, len(names)):
	f.write(names[i] + ',' + infos[i] + ',' + scors[i] + '\r\n')
f.close()

```

这里，老师的范例代码使用了上节课给大家提到过的fake UserAgent。最cool的用法是：
```python
from fake_useragent import UserAgent
# 别忘了首先需要安装：pip install fake-useragent
ua = UserAgent()
header = {"User-Agent": ua.random}
```
这样每次请求的UserAgent都是随机的，可以更好的“欺骗”服务器。

大家看看生成的文档，可以直接用Excel等表格工具打开，一目了然。对吧？
实际上的CSV也并不是特别简单，比如，如果数据内容中包含“,”逗号。那么需要想办法处理。事实上，老师的第一份工作，面试题就是写一个CSV的解析器。有兴趣的同学可以上网查查看。今天我们了解到这个地步就够了。

除了生成为CSV，还有个特别酷的办法，是用代码自动生成一个网页。下面我们看看：

## 创建网页
一个网页的最基本结构我们复习一下：
```html
<html>
	<head>
		<title>Page Name</title>
	</head>
	<body>
		Hello HTML!
	</body>
</html>
```
显然，在网页中我们希望用表格来保存电影数据，再复习一下表格：
```html
<table border="1">
	<tr>
  		<th>header1</th>
  		<th>header2</th>
	</tr>
	<tr>
  		<td>first line text</td>
  		<td>first line text</td>
	</tr>
	<tr>
  		<td>second line text</td>
  		<td>second line text</td>
	</tr>

</table>
```
有了这些知识已经够用了，我们看看把数据保存为一个网页(info2html.py)：

```python
f = open('movielist.html', 'w')
f.write('<html><head><title>Movie List</title></head><body>')
f.write('<table border="1">')
f.write('<tr><th>name</th><th>url</th><th>score</th></tr>')# title
for i in range(0, len(names)):
	f.write('<tr>')
	f.write('<td>' + names[i] + '</td>')
	f.write('<td>' + infos[i] + '</td>')
	f.write('<td>' + scors[i] + '</td>')
	f.write('</tr>')
f.write('</table></body></html>')
f.close()
```

大家看，其实并不复杂对吧？这个HTML页面还可否改进呢？
大家提提意见？
很好，我们可以让页面上直接生成链接，点击后就可以进入对应的详情页面。而不是简单的吧URL列出来。但是我们首先要看清楚URL是：
```python
/films/588362
```
这显然不是全部，我们观察一下网站，随便点开一个电影的详情页：
[https://maoyan.com/films/342965][6]
好的，很简单，完整的URL应该是：
```python
https://maoyan.com + url
```
再复习一下超链接的写法：
```html
<a href="http://www.example.com/">This is a Link</a>
```
改造代码如下（info2html2.py）：
```python

f = open('movielist2.html', 'w')
f.write('<html><head><title>Movie List</title></head><body>')
f.write('<table border="1">')
f.write('<tr><th>name</th><th>score</th></tr>')# title
for i in range(0, len(names)):
	f.write('<tr>')
	f.write('<td><a href="' + 'https://maoyan.com' + infos[i] + '">' + names[i] + '</a></td>')
	f.write('<td>' + scors[i] + '</td>')
	f.write('</tr>')
f.write('</table></body></html>')
f.close()

```

非常鼓励大家继续改造这个代码，让他看上去更酷！

## 作业
1. 尽可能让你输出的网页更加漂亮，研究一下，怎么装饰；
2. 找找我们之前讲过的命令行参数（参见Lecture15），以info2html2.py为基础，改造程序，通过提供不同的参数，让程序可以将爬虫数据保存成csv格式或者html格式。


[1]:	https://maoyan.com/films?showType=3&sortId=3
[2]:	https://maoyan.com/films?showType=3&sortId=3&offset=849330
[3]:	https://maoyan.com/films?showType=3&sortId=3&offset=849330
[4]:	https://baike.baidu.com/item/CSV/10739
[5]:	https://zh.wikipedia.org/wiki/%E9%80%97%E5%8F%B7%E5%88%86%E9%9A%94%E5%80%BC
[6]:	https://maoyan.com/films/342965