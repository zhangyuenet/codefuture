# 爬虫
## 教学目标
直观的了解一个爬虫的工作过程；

## 教学设计
### 课程导入
首先我们了解一下什么是“爬虫”：
爬虫：一段自动抓取互联网信息的程序，从互联网上抓取对于我们有价值的信息。
首先咱们聊聊为什么需要爬虫？
大家都知道从网上获取消息，截止到目前，互联网经历了三种获取信息的形态：
1. 通过一个所谓的门户网站（portal website）获取各种信息，这种方式特别像老人们读报纸，打开一份报纸，从天气预报到经济新闻到花边娱乐应有尽有了，在大陆，典型的门户网站是新浪网(sina.com.cn)；
2. 通过一个搜索引擎（search engine）获取需要的信息。这种方式避免了一种情况：你只想看某个体育比赛的消息，但是不得不在网站上浏览那些你不感兴趣的明星八卦或者股票信息。和第一种方式相比，最大的差别在于，获取信息变成了人的主动行为，我想要什么，我就去找什么。在大陆，典型的搜索引擎包括百度和bing，国外还有常用的google。
3. 通过主动的设置主题和关键词，或者通过机器自动学习，了解用户的兴趣爱好，推送获得消息。大家是不是有些体会，在浏览一些新闻的时候，网站经常会推送给我们一些我们感兴趣的消息。推送的规则可能是我们设置的，也可能是根据我们的阅读习惯，由机器推断出来的。和第二种信息获取方式（搜索）相比，最大的差异在于自动获取。在大陆，典型的以推送消息为服务的包括今日头条、腾讯新闻和网易新闻也越来越多的提供类似的服务。另外，各种音乐服务都会主动的推断用户的喜好，为用户推送符合其口味的音乐作品。

爬虫的一个特别重要的用途，就是针对第三种情形，根据用户的需求，主动的在网络上搜索相关的信息。你可以设想如下的场景：
- 你喜欢某位球星，可以通过一个爬虫在网络上主动搜索和该球星相关的比赛信息、球队动态、球星的最新行动等等，这样，你就可以时刻掌握最新的消息；
- 你即将参加一个重要的考试，可以利用一个爬虫，从网络自动的获取关于这个考试最新的消息和模拟题，有了这些信息，就省掉了自己频繁上网频搜索的麻烦。
爬虫最基本的来说，有这样几个步骤：
1. 分析数据：为爬虫设定目标范围；
2. 通过网络获取足够的数据；
3. 提取数据，获取有需要的部分。
下面我们通过一个例子详细的说说这三个步骤：

### 分析数据
上节课我们已经分析过了一个网页，现在我们再把它拿出来考察一下：
[https://maoyan.com/films?showType=3][1]
这个链接提供了经典电影的信息。网站的结构非常适合做我们开始学习爬虫的起点。所以今天，我们主要就是通过这个网站提供的数据来联系写爬虫。
这个网页中，大家看到有很多分类条件，比如按照类型，区域等。作为例子，我们选择全部类型，并选择按照评价排序。这样我们希望能够得到历史上评分最高的电影列表（当然，猫眼电影提供的评分未必很客观，老师并不完全同意，不过这不影响我们学习，大家学会后，可以去其它评分网站获取自己认可的评价列表）。

我们用下面的程序先把网页抓下来（webp1.py）：
```python
import requests

url = 'https://maoyan.com/films?showType=3'
html = requests.get(url).text
print(html)
```

我们看看打印出来的东西，好像和我们设想的不一样。老师得到的HTML代码如下：
```html
<html>
<head><title>403 Forbidden</title></head>
<body bgcolor="white">
<center><h1>403 Forbidden</h1></center>
<hr><center>openresty</center>
</body>
</html>

```
根据上节课学到的一点知识，我们至少能看到403 Forbidden的字样。403是一个HTTP的状态码，关于状态码，我们以后会学到，今天无需纠结。只需要知道我们被“Forbidden”了。这是怎么回事呢？

原来，很多网站的设计者为了保障网站浏览的顺畅，都会设置障碍防止我们这样的机器人去抓取信息，从而保障真实的用户浏览网站时不会变慢。
因此我们要学习一个非常重要的概念：用户代理UserAgent。
并且学会如何设置一个UserAgent，从而让网站认为我们是个真实的用户。

### UserAgent
UserAgent 包含了一个特征字符串，用来让网络协议的对端来识别发起请求的用户代理软件的应用类型、操作系统、软件开发商以及版本号。
简单的来说，在你浏览一个网站的时候，网站能够知道你用的是什么浏览器、电脑的操作系统，等等。例如，老师用的User-Agent如下所示：
```bash
Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko)
```
根据这个信息，网站就知道老师的是在一台Mac电脑上用Chrome浏览器上网。

这些信息对于网站正确的给用户显示内容是很重要的。如果它识别出来这是个爬虫，它就会禁止访问，并返回一个“403 Forbidden”。这种情况下我们应该怎么做呢？

【学生提问】我们用什么办法绕过UserAgent的限制呢？

真聪明，我们需要用一个普通用户浏览用的UserAgent设置为我们的，这样让网站“以为”我们是一个正常的用户在用浏览器浏览，这样就会允许我们获取信息。在写网络程序中，这是个非常常用的技巧。

首先，我们要获得一个UserAgent。如果你和老师一样在用Chrome浏览器，这很简单，在地址栏输入下面的指令：
[about:version][2]
就可以类似于老师的UserAgent字符串。

如果是IE浏览器，也很简单。用F12或者菜单打开开发人员工具。在工具栏中选择“网络”，随便访问一个网站，在列表中选择对应的GET记录，在右侧的请求标头中就可以看到如下的信息：
```bash
User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko
```
这是老师的一台Windows10电脑上的IE11的。其它浏览器也可以用类似的办法获得UserAgent字符串。

得到字符串后，改造我们的代码如下（webp2.py）：
```python
import requests

url = 'https://maoyan.com/films?showType=3&sortId=3'
header = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko)"}
html = requests.get(url, headers=header).text
print(html)
```

注意，这里的UserAgent是老师自己电脑的，你可以用这个，也可以用你自己的，也可以用从网络上搜索来的字符串。
再运行一下代码，看上去成功了。

在这里再给大家补充一个方便的“伪造”UserAgent的办法，因为用的人太多，所以Python有一个组件专门用来制作UserAgent字符串：fake-useragent。安装方法和其他第三方库一样，pip install fake-useragent即可。

使用方法也十分简单，导入类中的gf类，然后在使用random方法就可以获得一个User-Agent了，还支持指定浏览器的User-Agent功能。

这个老师不做详细讲解，按照大家的能力，应该可以自己尝试一下的。

### 寻找URL规律获得多个页面
获取数据的工作还没完成，为什么？因为网站并没有吧所有的电影信息都放在一个页面中，它是分页的。如果我们想获取的电影数据不止一页，那就需要让程序自动抓取多个页面的数据。一个理所当然的想法是，我们手动的浏览一遍，把所有的页面链接地址输入到程序中，让程序处理，但这样做，第一很笨拙，第二如果地址有变化，我们就要重新输入一遍，这太麻烦了。
如果我们想自动的获取更多的页面，就要动点脑筋，今天的课程，我们简单起见，只给大家演示如何获取十页数据。如何获取全部页面，会当成思考题留给大家。

为了获取多个页面，我们需要分析页面和地址之间的规律。
第一个页面是：
[https://maoyan.com/films?showType=3&sortId=3&offset=][3]
下一页是：
[https://maoyan.com/films?showType=3&sortId=3&offset=30][4]
下一页是：
[https://maoyan.com/films?showType=3&sortId=3&offset=60][5]
再在一页是：
[https://maoyan.com/films?showType=3&sortId=3&offset=90][6]

大家看出规律了么？大家猜一下，下一页的链接地址是什么？
答对了！页面地址的规律是：第N页的地址 offset=N乘以30，N从0开始。而前面的部分都是一样的，所以十页的信息可以这样获取（webp3.py）：
```python
import requests

url = 'https://maoyan.com/films?showType=3&sortId=3&offset='
header = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko)"}
html = requests.get(url, headers=header).text
for i in range(0, 9):
	p_url = url + str(i * 30)
	print(p_url)
```

大家观察一下，打印出来的十个链接是不是正确的？

### 分析内容
我们有办法自动的获取多个页面，下面就来看看如何从获取到的数据中得到我们想要的信息。
在这里我们做个约定，我们只关心，电影的名称，评分，还有详情链接。
我们拿第一个页面为例子，给大家解释如何获取信息。然后大家就可以从多个页面中抓取信息了。
我们再次运行一下webp2.py打印出页面数据。或者在浏览器中选择查看源代码。虽然数据量非常大，但是我们可以通过浏览内容知道，每一部电影的信息都成类似下面的结构了：
```html
    <div class="movie-item">
      <a href="/films/588362" target="_blank" data-act="movie-click" data-val="{movieid:588362}">
      <div class="movie-poster">
        <img class="poster-default" src="//s3plus.meituan.net/v1/mss_e2821d7f0cfe4ac1bf9202ecf9590e67/cdn-prod/file:5788b470/image/loading_2.e3d934bf.png" />
        <img data-src="https://p0.meituan.net/movie/aeb864fa21d578d845b9cefc056e40cb2874891.jpg@160w_220h_1e_1c" alt="摔跤吧！爸爸海报封面" />
      </div>
      </a>
      <div class="movie-ver"></div>
    </div>
    <div class="channel-detail movie-item-title" title="摔跤吧！爸爸">
      <a href="/films/588362" target="_blank" data-act="movies-click" data-val="{movieId:588362}">摔跤吧！爸爸</a>
    </div>
<div class="channel-detail channel-detail-orange"><i class="integer">9.</i><i class="fraction">8</i></div>
```
虽然有很多标签大家未必知道是做什么用的，但是至少知道类似于`<div></div>`是标签，而`class="movie-item"`是属性。每部电影的标题title属性，都在如下的标签内：
```html
<div class="channel-detail movie-item-title" title="摔跤吧！爸爸">
```
详情链接都在：
```html
 <a href="/films/588362" target="_blank" data-act="movies-click" data-val="{movieId:588362}">摔跤吧！爸爸</a>
```
而评分存在这里：
```html
<div class="channel-detail channel-detail-orange"><i class="integer">9.</i><i class="fraction">8</i></div>
```
注意，评分分成了两部分，整数部分和小数部分，合在一起才是准确评分。

为了让程序获取上面的数据，我们首先引入一个新的组件：lxml。
我们需要用到它的一个重要功能etree。大家用PIP先安装lxml。然后就可以在代码中使用了。下面的代码帮助大家从HTML文件中提取所有的电影title和详情链接地址（webp4.py）：
```python
import requests
from lxml import etree

url = 'https://maoyan.com/films?showType=3&sortId=3&offset='
header = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_2) AppleWebKit/537.36 (KHTML, like Gecko)"}
html = requests.get(url, headers=header).text

# 名称，评分，详情
data = etree.HTML(html)
names = data.xpath('//div[@class="channel-detail movie-item-title"]/@title')
infos = data.xpath('//div[@class="channel-detail movie-item-title"]/a/@href')

print(len(names), names)
print(len(infos), infos)
```
大家看到，关键的一步是用lxml中的etree对HTML文件进行了分析。然后用一个叫做XPATH的函数将数据提取成list。
Xpath是一种专门用于在HTML中检索内容的工具，它规则非常复杂，在这里老师只能给大家稍微解释一点点。大家注意以下两个规则：
```python
'//div[@class="channel-detail movie-item-title"]/@title'
'//div[@class="channel-detail movie-item-title"]/a/@href'
```
大家看看，再“望文生义”的猜猜看，
```python
div[@class="channel-detail movie-item-title"]
```
指示了程序定位到有对应class信息的div。然后 @title指定要获取对应的属性值。
```python
//div[@class="channel-detail movie-item-title"]/a/
```
定位到了div的下一层级`<a></a>`。通过@href获取对应的属性值。

运行一下上面的代码，看看数据是否正确？

下面到了相对麻烦的部分，因为评分数据存在两个值里面，需要把它们拼接起来，有两种办法，先说容易理解的，也是比较“笨”的办法。
我们首先要定位到有评分在的标签：
```python
'//div[@class="channel-detail channel-detail-orange"]/i[@class="integer"]/text()'
```
不同于用@item获取属性的值，获取标签的值用的是text()。

【学生提问】在这里问大家一个问题，上面的过滤方式还可否简化呢？
如果学生发懵，在这里给大家解释一下，过滤方式只需要能够取得有效的标记就可以。因为所有的评分数据都存在`i[@class="integer"] `中，所以，在这个页面的情况，也可以这样写：
```python
'//i[@class="integer"]/text()'
```
这样获取的数据和上面的写法获得的信息是一样的。大家体会一下。
小数部分这样获取：
```python
'//div[@class="channel-detail channel-detail-orange"]/i[@class="fraction"]/text()'
```
或者：
```python
'//i[@class="fraction"]/text()'
```

所以获得完整评分列表的代码类似于这样：
```python
# options 1
scores_int = data.xpath('//i[@class="integer"]/text()')
scores_fra = data.xpath('//i[@class="fraction"]/text()')

for i in range(0, len(scores_int) ):
	scores_int[i] = scores_int[i] + scores_fra[i]
print(len(scores_int), scores_int)
```

刚才说了这是个“笨办法”。是因为Xpath提供了一个语法，将一个标签下所有的数据都取出来并且拼接在一起。对于今天的评分数据，这正是我们想要的。代码很简单：
```python
# options 2
scores = data.xpath('//div[@class="channel-detail channel-detail-orange"]')
scors = [x.xpath('string(.)') for x in scores]
print(len(scors), scors)
```

到此，我们获取了想要的信息，有了这些信息，我们就可以将其利用成我们自己需要的样子。

## 作业
1. 上面的代码只是分析了一页的数据，我们获得了网站上评分排行前三十位的电影列表。我们也学习了如何获得前十页。结合起来的话，我们就可以获得一个大的列表，列表中包含了网站上排名前300的电影名称、评分和详情地址，尝试一下。
2. 【进阶可选】我们在课程中固定的只取了十页数据，想想看，结合最后一部分分析页面的方法。可否通过页面提供的信息，让程序知道网站一共有多少个页面，并且将这些页面的链接地址都取到？
	这个有些难度，大家可以尝试一下，如果不会也不用急，我们下节课会讲到。

[1]:	https://maoyan.com/films?showType=3
[2]:	about:version
[3]:	https://maoyan.com/films?showType=3&sortId=3&offset=
[4]:	https://maoyan.com/films?showType=3&sortId=3&offset=30
[5]:	https://maoyan.com/films?showType=3&sortId=3&offset=30
[6]:	https://maoyan.com/films?showType=3&sortId=3&offset=30